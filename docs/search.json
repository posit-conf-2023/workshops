[
  {
    "objectID": "workshops/gen-art/index.html",
    "href": "workshops/gen-art/index.html",
    "title": "Steal like an Rtist: Creative Coding in R",
    "section": "",
    "text": "Description\nR is a tool for data analysis, but also can be used for self expression. This workshop will be an introduction to creative coding in R in order to make visual art. We will take an inspiration-first approach, using compelling pieces to discuss and learn the techniques that shape the work. This workshop takes guidance from its namesake, the book “Steal Like An Artist” by Austin Kleon - once we have identified and learned to recreate existing works, we will cover how to take this inspiration and transform, remix, or reinterpret it in the pursuit of developing our own work and artistic styles. \n\nThis workshop is hands on and will cover colour theory and manipulation, a reintroduction of the data frame as the foundation for creating art (instead of just for analyzing data!), using ggplot2 as an artistic canvas, creating basic and specialized shapes, tiling and pattern making, developing your own functions and using iteration. We will also discuss how to use controlled randomness to convert a standalone piece into a generative art system that can produce many distinct outputs. Creative coding may seem a world apart from data analysis, but we see a large overlap and intersection of the skills used in both, not to mention the creative muscles that are already used in data visualization.\n\n\nAudience\nThis workshop is for you if you:\n\nare comfortable with R and RStudio, experience with tidyverse and ggplot2,\nare interested in applying data visualization skills more creatively, but may not know where to start or how to develop style/inspiration, and\nare an artist interested in exploring code as another medium for creating their work,\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nIjeamaka Anyene Fumagalli is a full-time senior data analyst in the healthcare space and a free time computational artist. Using code, mainly the R programming language, she creates data art and generative art systems. She loves all things grid based, minimal, and geometric which naturally led to her love for textile arts as well.\n\n\n\n\nSharla Gelfand is a freelance statistician, software developer, and artist with interests in web development, generative art, and textile arts. Their work explores the play between art created by a computer and by hand, creating generative art that looks like it could have been made by hand (like a textile, drawing, or painting), and making physical art based on data or outputs of a generative system, exploring the strengths and limitations of each."
  },
  {
    "objectID": "workshops/ml-python/index.html",
    "href": "workshops/ml-python/index.html",
    "title": "Machine Learning and Deep Learning with Python",
    "section": "",
    "text": "Description\nIn this workshop, you will learn the machine and deep learning fundamentals using a modern open-source stack. We’ll start with a brief introduction to Python’s scientific computing libraries, including NumPy, Pandas, and Matplotlib, which provide the foundation for data analysis and visualization. From there, we will dive into the scikit-learn API, a user-friendly, open-source library for machine learning in Python. You will learn how to use it to create machine learning classifiers and apply tree-based models like random forests, gradient boosting, and XGBoost.\nIn the second part of this workshop, we will also cover deep learning concepts and introduce PyTorch, the most widely used deep learning research library. You will also learn about training multi-layer neural networks efficiently using multi-GPU and mixed-precision techniques. Finally, we will explore how to use a pretrained large language transformer with scikit-learn and fine-tune it on a custom downstream task using PyTorch.\nBy the end of this workshop, you will have a good understanding of the fundamental principles of machine learning and be able to construct advanced classification pipelines for tabular data using scikit-learn. Additionally, you will gain experience in image classification and natural language processing techniques using PyTorch and be able to implement them in your own predictive modeling projects effectively.\n\n\nAudience\nThis workshop is for you if you:\n\nare a researcher, programmer, or engineer who wants to apply machine learning and deep learning,\nare a scientific computing practitioner who is interested in predictive modeling, or\nare a problem solver who want to learn about Python-tools for tabular, image, and text data.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nSebastian Raschka is a machine learning and AI researcher with a strong passion for education. As Lead AI Educator at Lightning AI, he is excited about making AI and deep learning more accessible and teaching people how to utilize these technologies at scale. Before dedicating his time fully to Lightning AI, Sebastian held a position as Assistant Professor of Statistics at the University of Wisconsin-Madison, where he specialized in deep learning and machine learning research. Moreover, Sebastian loves open-source software and has been a passionate contributor for over a decade. Next to coding, he also loves writing and authored the bestselling “Python Machine Learning” book and “Machine Learning with PyTorch and Scikit-learn”. If you want to learn more about Sebastian and what he is currently up to, please visit his website at https://sebastianraschka.com."
  },
  {
    "objectID": "workshops/dataviz-storytelling/index.html",
    "href": "workshops/dataviz-storytelling/index.html",
    "title": "Designing Data Visualizations to Successfully Tell a Story",
    "section": "",
    "text": "Description\nCommunicating data through meaningful and easily accessible visualization is a critical competence for most data-related roles including data scientists, analysts, scientific researchers, and managers. A well-designed graphic is able to inform, spark engagement, explain patterns, and drive decisions and actions. At the same time, poor choices in the design process can complicate interpretation or even, intentionally or unintentionally, mislead the audience.\nThe aim of this course is to demystify the creative processes of data visualization design to turn data into a meaningful story. Participants will learn helpful tips and tricks to create appealing and informative data visualizations that are not “just showing the numbers” but successfully tell a story. We will cover principles of good data visualization design, explore different options to display data, and discuss ways to guide and engage viewers with the aim to create impactful graphics. The course also features sessions on picking suitable yet beautiful colors, what to consider when choosing and pairing typefaces, and the layout of graphics, also in the context of dashboard building.\n\n\nAudience\nThis course is for you if you:\n\nwant to understand and learn the art of communicating data with impactful visualizations,\naim to improve your data visualization design to create effective and informative graphics, and\nare willing to spend a bit more time to choose the right chart, proper color palettes, and suitable fonts along with additional elements to guide the viewer.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nCédric Scherer is a data visualization designer, consultant, and instructor helping clients and workshop participants to create engaging and effective graphics. As a graduated ecologist, he has acquired an extensive hypothesis–driven research experience and problem–solving expertise in data wrangling, statistical analysis, and model development. As an independent data visualization designer, Cédric later combined his expertise in analyzing large data sets with his passion for design, colors and typefaces. Cédric has designed graphics across all disciplines, purposes, and styles applying a code–first approach and regularly talks about data visualization design and ggplot2 techniques. Due to regular participation in social data challenges such as #TidyTuesday, he is now well known for complex and visually appealing figures, entirely made with ggplot2, that look as if they have been created with a vector design tool."
  },
  {
    "objectID": "workshops/intro-ds-r/index.html",
    "href": "workshops/intro-ds-r/index.html",
    "title": "Introduction to Data Science with R and Tidyverse",
    "section": "",
    "text": "Description\nThis is not a standard workshop, but a six-week online apprenticeship that culminates in two in-person days at posit::conf(2023). Here, you will learn the foundations of R and the Tidyverse under the guidance of a Posit Academy mentor and in the company of a close group of fellow learners. You will be expected to complete a weekly curriculum of interactive tutorials, and to attend a weekly presentation meeting with your mentor and fellow students. Topics will include the basics of R, importing data, visualizing data with ggplot2, wrangling data with dplyr and tidyr, working with strings, factors, and date-times, modelling data with base R, and reporting reproducibly with quarto. Begins August 7th, 2023. No knowledge of R required. Visit posit.co/academy to learn more about this uniquely effective learning format.\n\n\nAudience\nThis course is ideal for:\n\nthose new to R or the Tidyverse,\nanyone who has dabbled in R, but now wants a rigorous foundation in up-to-date data science best practices, or\nSAS and Excel users looking to switch their workflows to R.\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nPosit Academy is Posit’s internal team of data scientists and educators. We provide a cohort-based, mentor-led, hands-on data science apprenticeship for working professionals. It is delivered on an online platform with interactive coding exercises and frequent cross-student collaboration."
  },
  {
    "objectID": "workshops/modeling-deploy/index.html",
    "href": "workshops/modeling-deploy/index.html",
    "title": "Deploy and maintain models with vetiver",
    "section": "",
    "text": "Description\nMany data scientists understand what goes into training a machine learning or statistical model, but creating a strategy to deploy and maintain that model can be daunting. In this workshop, learn what MLOps (machine learning operations) is, what principles can be used to create a practical MLOps strategy, and what kinds of tasks and components are involved. We’ll use vetiver, a framework for MLOps tasks in Python and R, to version, deploy, and monitor the models you have trained and want to deploy and maintain in production reliably and efficiently.\n\n\nAudience\nWe expect participants to have exposure to basic modeling and machine learning practice, but NOT expert familiarity with advanced ML or MLOps topics.\nThis workshop is for you if you:\n\nhave intermediate R or Python knowledge (this will be a “choose your own adventure” workshop where you can work through the exercises in either R or Python),\ncan read data from CSV and other flat files, transform and reshape data, and make a wide variety of graphs, and\ncan fit a model to data with your modeling framework of choice.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nJulia Silge is a data scientist and software engineer at Posit PBC where she works on open source modeling and MLOps tools. She is an author, an international keynote speaker, and a real-world practitioner focusing on data analysis and machine learning.Julia loves text analysis, making beautiful charts, and communicating about technical topics with diverse audiences."
  },
  {
    "objectID": "workshops/tidymodels-intro/index.html",
    "href": "workshops/tidymodels-intro/index.html",
    "title": "Introduction to tidymodels",
    "section": "",
    "text": "Description\nThe workshop will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and basic pre-processing with recipes. Time permitting, you’ll be introduced to model optimization using the tune package. You’ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\n\n\nAudience\nThis workshop is for you if you:\n\nhave intermediate R knowledge, experience with tidyverse packages, and either of the R pipes,\ncan read data into R, transform and reshape data, and make a wide variety of graphs, and\nhave had some exposure to basic statistical concepts such as linear models, random forests, etc.\n\nIntermediate or expert familiarity with modeling or machine learning is not required.\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nHannah Frick is a software engineer and statistician on the tidymodels team at Posit. She holds a PhD in statistics and has worked in data science consultancy as well as interdisciplinary research at University College London in cooperation with Team GB Hockey.\n\n\n\n\nSimon Couch works on software for statistical modeling on the tidymodels team at Posit. With a background in statistics and sociology, Simon is passionate about free and open source software and data pedagogy. He is an author and maintainer of several R packages, including the stacks package, which was awarded the 2021 John M. Chambers Statistical Software Award.\n\n\n\n\nEmil Hvitfeldt is a software engineer at Posit and part of the tidymodels team’s effort to improve R’s modeling capabilities. He maintains several packages within the realms of modeling, text analysis, and color palettes. He co-authored the book Supervised Machine Learning for Text Analysis in R with Julia Silge."
  },
  {
    "objectID": "workshops/teach-ds/index.html",
    "href": "workshops/teach-ds/index.html",
    "title": "Teaching a modern data science course",
    "section": "",
    "text": "Description\nThere has been significant innovation in introductory statistics and data science courses to equip students with the statistical, computing, and communication skills needed for modern data analysis. Success in data science and statistics is dependent on the development of both analytical and computational skills, and the demand for educators who are proficient at teaching both these skills is growing. The goal of this workshop is to equip educators with concrete information on content, workflows, and infrastructure for painlessly introducing modern computation with R and RStudio within a data science curriculum.\nIn this workshop, we’ll present a modern approach to and workflows for teaching data science. We’ll discuss teaching the tidyverse in 2023, highlighting updates to the R for Data Science book as well as the tidyverse in general that should be reflected in today’s data science curricula as well as present tooling options and workflows for reproducible authoring, computing infrastructure, version control, and collaboration. In a nutshell, the day you’ll spend in this workshop will save you hours and hours of solo work setting up your course.\nThe workshop will be comprised of four modules:\n\nTeaching data science with the tidyverse and Quarto\nTeaching data science with Git and GitHub\nOrganizing, publishing, and sharing of course materials\nComputing infrastructure for teaching data science\n\nThroughout each module we’ll shift between the student perspective and the instructor perspective. The activities and demos will be hands-on; attendees will also have the opportunity to exchange ideas and ask questions throughout the session.\nIn addition to gaining technical knowledge, participants will engage in discussion around the decisions that go into developing a data science curriculum and choosing workflows and infrastructure that best support the curriculum and allow for scalability. We will also discuss best practices for configuring and deploying classroom infrastructures to support these tools.\n\n\nAudience\nThis workshop is aimed primarily at participants teaching data science in an academic setting in semester-long courses, however much of the information and tooling we introduce is applicable for shorter teaching experiences like workshops and bootcamps as well. Basic knowledge of R is assumed and familiarity with the tidyverse and Git is preferred.\nThis course is for you if you:\n\nyou want to learn / discuss curriculum, pedagogy, and computing infrastructure design for teaching data science with R and RStudio using the tidyverse and Quarto,\nyou are interested in setting up your class in Posit Cloud,\nyou want to integrate version control with git into your teaching and learn about tools and best practices for running your course on GitHub.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nDr. Mine Çetinkaya-Rundel (she/her) is Professor of the Practice at Duke University and Developer Educator at Posit. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM. Mine works on integrating computation into the undergraduate statistics curriculum, using reproducible research methodologies and analysis of real and complex datasets. Mine works on the OpenIntro project, whose mission is to make educational products that are free, transparent, and lower barriers to education. As part of this project she co-authored four open-source introductory statistics textbooks. She is also the creator and maintainer of datasciencebox.org and she teaches the popular Statistics with R MOOC on Coursera. Mine is a Fellow of the ASA and Elected Member of the ISI as well as the winner of the 2021 Robert V. Hogg Award for For Excellence in Teaching Introductory Statistics."
  },
  {
    "objectID": "workshops/arrow/index.html",
    "href": "workshops/arrow/index.html",
    "title": "Big Data with Arrow in R",
    "section": "",
    "text": "Description\nData analysis pipelines with larger-than-memory data are becoming more and more commonplace. In this workshop you will learn how to use Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data, to create seamless “big” data analysis pipelines with R.\nThe workshop will focus on using the the arrow R package—a mature R interface to Apache Arrow— to process larger-than-memory files and multi-file data sets with arrow using familiar dplyr syntax. You’ll learn to create and use interoperable data file formats like Parquet for efficient data storage and access, with data stored both on disk and in the cloud, and also how to exercise fine control over data types to avoid common large data pipeline problems. This workshop will provide a foundation for using Arrow, giving you access to a powerful suite of tools for performant analysis of larger-than-memory data in R.\n\n\nAudience\nThis course is for you if you:\n\nwant to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\nwant to learn about Parquet and other file formats that are powerful alternatives to CSV files\nwant to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nNic Crane is a software engineer with a background in data science, and has a lot of enthusiasm for open source and learning and teaching all things R. Nic is part of the core team who maintain the Arrow R package.\n\n\n\n\nSteph Hazlitt is a data scientist, researcher and R enthusiast. She has spent the better part of her career wrangling data with R and supporting people and teams in learning, creating and sharing data science-related products and open source software."
  },
  {
    "objectID": "workshops/shiny-r/index.html",
    "href": "workshops/shiny-r/index.html",
    "title": "Getting Started with Shiny for R",
    "section": "",
    "text": "Description\nShiny is an R package that makes it easy to build interactive web apps straight from R. This workshop will start at the beginning: designing and creating user interfaces, learning and mastering the reactive model that connects your R code to the interface, and deploying apps publicly and privately. We will wrap up with some intermediate-level tools: debugging and modularizing your apps and implementing dynamic user interfaces. In the end, you’ll be a confident Shiny user, able to design interactive apps to achieve your purpose and produce a polished and professional implementation.\n\n\nAudience\nThis course is for you if you:\n\nare comfortable with the basics of R, such as writing functions, indexing vectors and lists, debugging simple errors, and working with data structures like data frames,\nare interested in creating interactive web applications, and\nhave no or minimal experience with Shiny for R.\n\nIf you have a bit of experience, you’ll see things in a new way. If you don’t, we’ll get you started on the right footing.\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nColin Rundel is Associate Professor of the Practice at Duke University where he has been teaching since 2012. His work focuses on teaching statistical computing to both undergraduate and graduate students in both R and Python. He has been teaching and using Shiny since 2015."
  },
  {
    "objectID": "workshops/manage-os/index.html",
    "href": "workshops/manage-os/index.html",
    "title": "It’s not just code: managing an open source project",
    "section": "",
    "text": "Description\nAn open source project starts with ideas and code, but it continues with people. We know that most open source projects rely on just one or two people for most of the work. And code is just a small part of these roles, which also include project management, conflict resolution, decision making in uncertain situations, building an inclusive community, and lots and lots of communication. Whether you’re just starting a project, interested in getting involved in open source, or already have a community of thousands, there are some tips, tricks and templates that you can use to make maintaining an open source project more manageable.\nIn this interactive 1-day workshop you’ll learn some key practices for effect open source project management. You’ll walk away with new approaches for making project decisions, better strategies to manage user and contributor interactions, and better ways to set boundaries. Different projects make different decisions based on their needs and in this workshop you’ll see what types of decisions are possible, and how you can make the ones appropriate for you. During the workshop, you’ll create useful documents for your repository and begin to outline some processes you can finalize later or with your team.\n\n\nAudience\nThis workshop is for you if you:\n\nare involved in maintaining an open source project and struggling to feel like it’s sustainable, or are looking for practice or guidance,\nare starting out or interested in being involved in maintaining an open source project, and want to learn how to set up the project for the most effective engagement from contributors and users, or\nare interested in learning more about the people side of open source project maintenance and connecting with other maintainers.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nTracy Teal is the Open Source Program Director at Posit. Previously, she was a co-founder of Data Carpentry and the Executive Director of The Carpentries. She developed open source bioinformatics software as an assistant professor at Michigan State University and holds a PhD in computation and neural systems from California Institute of Technology. Tracy is involved in the open source software and reproducible research communities, including serving on advisory committees for NumFOCUS, pyOpenSci, EarthLab and carbonplan, and has been working with open source communities, developing curriculum, and teaching people how to work with data and code as a developer, instructor and project leader throughout her career."
  },
  {
    "objectID": "workshops/pkgdev-masterclass/index.html",
    "href": "workshops/pkgdev-masterclass/index.html",
    "title": "Package Development Masterclass",
    "section": "",
    "text": "Description\nIn this one-day masterclass, you’ll have the opportunity to dig deep into package development. We’ll cover a selection of cutting-edge topics (e.g. advanced testing tools, writing actionable error messages) that have significantly impacted the way we develop packages in the last couple of years. There will be plenty of time to apply what you learn to your own package, in an environment where you can get help and talk things through with the tidyverse team.\n\n\nAudience\nThis workshop is for you if you:\n\nhave already developed one or more packages, and are prepared to work on them during with workshop,\nare familiar with the basic devtools/RStudio workflow (e.g. load_all(), roxygen2, testthat, and usethis), and\nare familiar with Git.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nHadley Wickham is Chief Scientist at RStudio, winner of the 2019 COPSS award, and a member of the R Foundation. He builds tools (both computational and cognitive) to make data science easier, faster, and more fun. His work includes packages for data science (like the tidyverse, which includes ggplot2, dplyr, and tidyr)and principled software development (e.g. roxygen2, testthat, and pkgdown). He is also a writer, educator, and speaker promoting the use of R for data science. Learn more on his website, https://hadley.nz."
  },
  {
    "objectID": "workshops/admin/index.html",
    "href": "workshops/admin/index.html",
    "title": "Administering the Posit Professional Products",
    "section": "",
    "text": "Description\nAttendees will learn the best practices for how to administer the Posit professional products. You’ll learn the product architectures, options for monitoring and optimizing resource utilization, and the criteria you should consider as you scale. You’ll uncover the configurations that Posit Administrators can set to promote reproducible data science workflows. You’ll also learn how to scope and troubleshoot issues that may arise as your data scientists use the products. At the end of this workshop, Admins will better understand how they can best leverage the Posit professional products to serve their organization’s data science needs over time.\n\n\nAudience\nThis workshop is for you if you:\n\nare responsible for the administration of Posit Workbench, Posit Connect, or Posit Package Manager at your organization,\nhave experience with Linux and can confidently navigate the file system, manage users and permissions, and read logs, and\nwant to learn the best practices for administering the Posit products so that your team can get the most out of the products now and into the future. \n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nShannon Hargerty is a Solutions Engineer at Posit, where she helps organizations determine how to optimize their configuration and use of the Posit Products for their use case. Before coming to Posit, Shannon earned a Ph.D. in Ecology and taught R and Python researchers."
  },
  {
    "objectID": "workshops/shiny-dashboard/index.html",
    "href": "workshops/shiny-dashboard/index.html",
    "title": "Shiny Dashboards",
    "section": "",
    "text": "Description\nIn this workshop we will explore all of the interesting and variety ways you can use shiny: from adding dynamic elements to your existing RMarkdown / Quarto documents, to building and deploying dashboards for reporting, and customizing the appearance and themeing of the app (and your outplots like plots and tables). This workshop assumes that you have a basic familiarity with Shiny (e.g. the ability to write simple apps and basics of reactivety).\n\n\nAudience\nThis course is for you if you:\n\nhave some experience with Shiny and want to improve your skills,\nare interested in building dashboards for reporting, and\nwant to learn about styling and theming your dashboards.\n\nUsers who are new to Shiny will benefit from taking Getting Started with Shiny for R before joining this workshop.\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nColin Rundel is Associate Professor of the Practice at Duke University where he has been teaching since 2012. His work focuses on teaching statistical computing to both undergraduate and graduate students in both R and Python. He has been teaching and using Shiny since 2015."
  },
  {
    "objectID": "workshops/pkgdev/index.html",
    "href": "workshops/pkgdev/index.html",
    "title": "Fundamentals of Package Development",
    "section": "",
    "text": "Description\nWe are often faced with the need to share our code with others, or find ourselves writing similar code over and over again across different projects. In R, the fundamental unit of reusable code is a package, containing helpful functions, documentation, and sometimes sample data. This workshop will teach you the fundamentals of package development in R, using tools and principles developed and used extensively by the tidyverse team - specifically the ‘devtools’ family of packages including usethis, testthat, and roxygen2. These packages and workflows help you focus on the contents of your package rather than the minutiae of package structure. \nYou will learn the structure of a package, how to organize your code, and workflows to help you develop your package iteratively. You will learn how to write good documentation so that users can learn how to use your package, and how to use automated testing to ensure it is functioning the way you expect it to, now and into the future. You will also learn how to check your package for common problems, and how to distribute your package for others to use.\nThis will be an interactive 1-day workshop, and we will be using the RStudio IDE to work through the materials, as it has been designed to work well with the development practices we will be featuring.\n\n\nAudience\nThis workshop is for you if:\n\nYou have written several R scripts and find yourself wondering how to reuse or share the code you’ve written;\nYou know how to write functions in R\nYou are looking for a way to take the next step in your R programming journey\n\nWe will be demonstrating some workflows using Git and GitHub. Knowledge of these tools is not required, and you will absolutely be able to complete the workshop without them, but some of the lessons will be more rewarding to you if you are prepared to try them out. If you are looking to get started with Git and GitHub, we recommend you register for the “What they forgot to teach you about R” workshop on Day 1, and join us for this workshop on Day 2.\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nAndy Teucher is an R Package Developer Educator at Posit.\n\n\n\nHis background is in conservation biology, with an MSc in Ecology from the University of Calgary. He has spent much of his career as a data scientist in government, where he made it his mission to promote and teach open, reproducible data science practices. He has written R packages for internal use in his teams as well as for a broader audience, with several hosted on CRAN. Andy especially enjoys developing packages to make it easier to work with spatial data in R. He has a passion for teaching others to learn how to use data science tools to make their work more efficient and reproducible. Andy is a certified Software Carpentry and Data Carpentry instructor, and has led many workshops teaching programming skills to scientists and data professionals. |\n: {tbl-colwidths=“[25,5,70]”}"
  },
  {
    "objectID": "workshops/shiny-python/index.html",
    "href": "workshops/shiny-python/index.html",
    "title": "Getting Started with Shiny for Python",
    "section": "",
    "text": "Shiny for Python is a new technology for automating business analysis and data science for organizations that use Python and want to get applications into production in days (not months or years). In this one-day workshop, you will learn how to progressively build a Shiny for Python application using a “4-Step Shiny for Python Build Framework” on a real business case study that benefits both organizations and individuals. Students first discover how to integrate a financial API using yfinance, a popular python library for retrieving company financial data, into a Shiny for Python web application. Students then learn how to apply the “4-Step Shiny for Python Build Framework” to automate the complete financial analysis for any of the 507 stocks listed on the S&P 500 index. Students leave with the code, knowledge, and a complete framework for building Shiny applications in Python.\n\n\n\nWhat if I’m a complete beginner?\n\nIt’s beneficial to have used R or Python before. With that said, all concepts will be explained thoroughly. \n\nWhat if I’ve never built a Shiny app before?\n\nThat’s perfect. I’ll show you how with my 4-step framework. \n\nDo I need experience working with financial data?\n\nNo prior financial experience is required."
  },
  {
    "objectID": "workshops/quarto-r-projects/index.html",
    "href": "workshops/quarto-r-projects/index.html",
    "title": "Advanced Quarto with R and RStudio: Projects, Websites, Books, and More",
    "section": "",
    "text": "Description\nThis workshop will prepare you to author a rich array of documents in Quarto, the next generation of R Markdown. Quarto is an open-source scientific and technical publishing system that offers multilingual programming language support to create dynamic and static documents, books, presentations, blogs, and other online resources.\nThe focus for this workshop will be on projects that weave together multiple documents and allow you to write books and build websites. You will also learn various ways to deploy and publish your Quarto projects on the web.\n\n\nAudience\nThis course is for you if you:\n\nhave a basic knowledge of how to use the RStudio IDE.\nare excited to author a range of reproducible computational documents types, from static files to slides to websites.\n\nParticipants who are new to computational documents will benefit from taking Intro to Quarto with R and RStudio: Documents and Presentations before joining this workshop.\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nAndrew Bray is an Associate Teaching Professor in the Department of Statistics at UC Berkeley where he develops and teaches courses in statistics and data science. His research interests include statistical computing, data privacy, and applications of statistical models to solve real world problems. He was previously an Associate Professor of Statistics in the Department of Mathematics at Reed College and an NSF Five Colleges postdoctoral fellow in western Massachusetts."
  },
  {
    "objectID": "workshops/quarto-python/index.html",
    "href": "workshops/quarto-python/index.html",
    "title": "Enhancing Communication & Collaboration in Data Science with Quarto and Jupyter Notebooks",
    "section": "",
    "text": "Description\nSharing knowledge through writing is a critical aspect of scientific activity, including data science. It allows researchers to communicate their findings and insights to a wider audience, build upon existing work, and collaborate with others in their field. However, until recently, there have been limited options for publishing long-form writing and expository analyses authored in Jupyter Notebooks, a popular medium for data scientists.\nEnter Quarto - an innovative, open-source scientific and technical publishing system compatible with Jupyter Notebooks and other popular mediums. Quarto provides data scientists with a seamless way to publish their work in a high-quality format that is easily accessible and shareable. With Quarto, researchers can turn their Jupyter Notebooks into professional-looking publications in a variety of formats, including web pages, books, and slides.\nIn this workshop, we will demonstrate how Quarto enables data scientists to turn their work products into professional, high-quality publications, websites, blog posts, and other shareable artifacts. As a bonus, we will also discuss how you can create and document Python packages using Jupyter notebooks and Quarto with the help of nbdev.\nThe learning outcomes for the workshop include:\n\nexamine case studies where sharing scientific knowledge has greatly improved the efficacy of data science teams.\nauthor documents in plain text markdown or Jupyter notebooks with equations, citations, crossrefs, figure panels, callouts, and advanced layouts.\nlearn how to author content in IPython/Jupyter and the Quarto VS Code extension.\nleverage Quarto for creating different types of publications, including personal blogs, knowledge management for teams, notes, books, websites, and presentation slides.\nextend Quarto with notebook filters and extensions.\nhost websites and publications on platforms like GitHub Pages, QuartoPub, and Netlify.\ntest notebooks and documentation with Quarto’s execution options.\ncreate and document Python packages with nbdev and Quarto.\n\n\n\nAudience\nThis workshop is for you if you:\n\nhave some experience with Python and Jupyter and want to learn how Quarto can support and enhance your workflows,\nwant to learn about turning your notebooks to websites and publications and\nwant to learn how to write python packages with Jupyter notebooks and Quarto with the help of nbdev.\n\nThe workshop will assume some prior experience with Python and Jupyter Notebooks.\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nHamel Husain. Hamel is an entreprenuer-in-residence at fast.ai, where he is building new software development tools like nbdev that leverages Quarto. Prior to fast.ai, Hamel was a machine learning engineer at companies like Airbnb, GitHub, and DataRobot, and other related roles in management consulting. You can find more about Hamel on his personal site."
  },
  {
    "objectID": "workshops/ds-workflows-r/index.html",
    "href": "workshops/ds-workflows-r/index.html",
    "title": "Data Science Workflows with Posit Tools — R Focus",
    "section": "",
    "text": "Description\nIn this R-focused workshop, we will discuss ways to improve your data science workflows! During the course, we will review packages for data validation, alerting, modeling, and more. We’ll use Posit’s open source and professional tools to string all the pieces together for an efficient workflow. We’ll discuss environments, managing deployed content, working with databases, and interoperability across data products.\n\n\nAudience\nThis course is for you if you:\n\nBuild finished data products starting from raw data and are looking to improve your workflow\nAre looking to expand your knowledge of Posit open source and professional tools\nWant to improve interoperability between data products in your work or on your team\nHave experience developing in R. A analogous course with a Python focus is also offered\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nRyan Johnson is a Data Science Advisor at Posit with a background in Microbiology and Bioinformatics. He obtained his PhD from the Uniformed Services University in Maryland and did his postdoctoral training at the National Human Genome Research Institute, NIH. The only thing that rivals his love for infectious diseases is generating ‘super cool’ visualizations from large data sets using R and RStudio. In his free time, you can find Ryan running marathons/ultramarathons in the DC area or hiking miles along the Appalachian Trail. Ryan resides in Gaithersburg with his wife and two feline co-workers.\n\n\n\n\nKatie Masiello is a Solutions Engineer at Posit. A mechanical engineer by training, she found her calling in data science while working statistical analysis in the aerospace industry. A good cup of coffee, reproducibility, and making life easier for the next user are three things she loves most. Katie is an avid knitter and knitr, and she can often be found trying to tame her ridiculously overgrown garden, collecting pebbles, or thinking about taking up running as a hobby."
  },
  {
    "objectID": "workshops/programming-r/index.html",
    "href": "workshops/programming-r/index.html",
    "title": "From R User to R programmer",
    "section": "",
    "text": "Description\nThis is a one-day, hands-on workshop for those who have embraced the tidyverse and want to improve their R programming skills and, especially, reduce the amount of duplication in their code. The two main ways to reduce duplication are creating functions and using iteration. We will use a tidyverse approach to cover function design and iteration with purrr.\nIn this workshop you will master the art of writing functions that do one thing well, adhere to existing conventions and can be fluently combined together to solve more complex problems. You will also learn how to perform the same action on many objects using code which is succinct and easy to read.\n\n\nAudience\nThis course is for you if you:\n\nhave experience equivalent to an introductory data science course using tidyverse\nfeel comfortable with the Whole game chapter of R for Data Science\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nEmma Rand is a Senior Lecturer in the Department of Biology at the University of York where she specializes in teaching data science and reproducibility, particularly to those who do not see themselves as programmers. She leads a UKRI funded project called Cloud-SPAN which trains researchers in cloud-based high performance computing for ’omics. She is a Software Sustainability Institute Fellow, a Teaching team lead for R Forwards and delivers data science training for the Royal Society of Biology and the Biochemical Society.\n\n\n\n\nIan Lyttle is a Data Scientist at Schneider Electric. His technical interests include visualization, interactivity, and functional programming. He is a community contributor to tidyverse and r-lib, and maintains CRAN packages including vegawidget and boxr. He has delivered tutorials on a variety of R topics at UseR!, Uncoast Unconf, and the Iowa State University Graphics Group."
  },
  {
    "objectID": "workshops/shiny-ui/index.html",
    "href": "workshops/shiny-ui/index.html",
    "title": "Web Design for Shiny Developers",
    "section": "",
    "text": "Description\nWebsite design and development is one of the most critical factors contributing to whether the user has a good or poor experience while browsing your site, directly influencing the overall impression of your brand. Besides, bad design decisions can significantly impact app performances. By exposing you to common governing rules of design, this course will walk you through the entire design process, from ideation to execution. These rules will help you to become a better collaborator to design teams, and enable you to create beautiful front-end experiences for Shiny.\n\n\nAudience\nThis course is for you if:\n\nyou are an R developer with basic Shiny knowledge\nyou want to quickly test new business ideas\nyou want to increase the reach of your apps and websites\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nDavid Granjon holds a PhD in applied mathematics from Université Pierre et Marie Curie and Université de Lausanne. He is the founder and maintainer of the open source RinteRface organisation (https://rinterface.com) where he develops Shiny extensions such as {bs4Dash}, {shinyMobile} or {shinydashboardplus} and delivers novel advanced Shiny workshops in worldwide R conferences like useR or R in Pharma. David works as a full stack software developer at Novartis where he provides his expertise to help associates to design production ready shiny apps, from ideation to headless testing and automated deployment with CICD. He wrote Outstanding User Interfaces with Shiny”, a CRC press book published in 2022 (bookdown version: https://unleash-shiny.rinterface.com/, book: https://www.routledge.com/Outstanding-User-Interfaces-with-Shiny/Granjon/p/book/9780367643652).\n\n\n\n\nMaya Gans is a Data Visualization Engineer at Atorus Research where she designs and develops custom applications using R, JavaScript, or a combination of both. As an intern at RStudio Maya designed TidyBlocks, a visual block based programming language. Maya also co-wrote JavaScript for Data Science. When Maya isn’t designing or programming, she is rock climbing."
  },
  {
    "objectID": "workshops/dataviz-ggplot2/index.html",
    "href": "workshops/dataviz-ggplot2/index.html",
    "title": "Engaging and Beautiful Data Visualizations with ggplot2",
    "section": "",
    "text": "Description\nCreating effective and easily accessible data visualizations of high quality in an efficient and preferably reproducible way is an essential skill for everyone working in a data-related field. Luckily, by leveraging the functionality of ggplot2, the most famous package for data visualization with R, and related extension packages one can create highly customized data visualization without the need for post-processing.\nThis workshop provides everything one needs to know to create and customize numerous chart types with ggplot2. Participants will learn the most important steps and helpful tips to create visually appealing and informative graphics with a code-only approach. The power of ggplot2 and related extension packages will be illustrated with advanced real–life examples that help to understand useful coding tricks and the process of creating engaging and effective visualizations. The workshop will particularly focus on more advanced tasks with ggplot2 such as styling labels and titles, customizing themes and visual aesthetics, and using less-common chart types.\n\n\nAudience\nThis course will be appropriate if you:\n\nalready know how to create basic graphics with the ggplot2 package,\naim to improve the design of your ggplot outputs, and\nwant to learn how to create more complex charts which feature multiple layers, annotations, text styling, custom themes, and more.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nCédric Scherer is a data visualization designer, consultant, and instructor helping clients and workshop participants to create engaging and effective graphics. As a graduated ecologist, he has acquired an extensive hypothesis–driven research experience and problem–solving expertise in data wrangling, statistical analysis, and model development. As an independent data visualization designer, Cédric later combined his expertise in analyzing large data sets with his passion for design, colors and typefaces. Cédric has designed graphics across all disciplines, purposes, and styles applying a code–first approach and regularly talks about data visualization design and ggplot2 techniques. Due to regular participation in social data challenges such as #TidyTuesday, he is now well known for complex and visually appealing figures, entirely made with ggplot2, that look as if they have been created with a vector design tool."
  },
  {
    "objectID": "workshops/ds-workflows-python/index.html",
    "href": "workshops/ds-workflows-python/index.html",
    "title": "Data Science Workflows with Posit Tools — Python Focus",
    "section": "",
    "text": "Description\nIn this Python-focused workshop, we will discuss ways to improve your data science workflows! During the course, we will review packages for data validation, alerting, modeling, and more. We’ll use Posit’s open source and professional tools to string all the pieces together for an efficient workflow. We’ll discuss environments, managing deployed content, working with databases, and interoperability across data products.\n\n\nAudience\nThis course is for you if you:\n\nBuild finished data products starting from raw data and are looking to improve your workflow\nAre looking to expand your knowledge of Posit open source and professional tools\nWant to improve interoperability between data products in your work or on your team\nHave experience developing in Python. A analogous course with an R focus is also offered\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nGagan Singh is a former software engineer and data scientist who has worked in a variety of cross-technology teams. Before joining Posit as a Solutions Engineer, he was consulting with fortune 500 companies to build their analytical capacities. Outside of work you can find him at a local bookstore or exploring the beautiful Pacific Northwest.\n\n\n\n\nSam Edwardes is obsessed with data science, R, Python, and all things open source. As a Solutions Engineer at Posit, he loves getting into the nitty gritty and optimizing everything. When he is not tinkering on his computer you can find Sam and his dog Roo on adventures in beautiful British Columbia."
  },
  {
    "objectID": "workshops/intro-ds-python/index.html",
    "href": "workshops/intro-ds-python/index.html",
    "title": "Introduction to Data Science with Python",
    "section": "",
    "text": "Description\nThis is not a standard workshop, but a six-week online apprenticeship that culminates in two in-person days at posit::conf(2023). Here, you will learn the foundations of Python and data analysis under the guidance of a Posit Academy mentor and in the company of a close group of fellow learners. You will be expected to complete a weekly curriculum of interactive tutorials, and to attend a weekly presentation meeting with your mentor and fellow students. Topics will include importing packages and datasets, visualizing data with plotnine, wrangling data with pandas, writing and applying functions, and reporting reproducibly with quarto. Begins August 7th, 2023. No knowledge of Python required. Visit http://posit.co/academy to learn more about this uniquely effective learning format.\n\n\nAudience\nThis course is ideal for:\n\nthose new to Python,\nanyone who has dabbled in Python, but is not sure how to use Python to do data science, or\nR users who want to work more closely with Python users on their team.\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nPosit Academy is Posit’s internal team of data scientists and educators. We provide a cohort-based, mentor-led, hands-on data science apprenticeship for working professionals. It is delivered on an online platform with interactive coding exercises and frequent cross-student collaboration."
  },
  {
    "objectID": "workshops/tidymodels-advanced/index.html",
    "href": "workshops/tidymodels-advanced/index.html",
    "title": "Advanced tidymodels",
    "section": "",
    "text": "Description\nYou will learn more about model optimization using the tune and finetune packages, including racing and iterative methods. You’ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods. Participants who have completed the “Introduction to tidymodels” workshop will be well-prepared for this course.\n\n\nAudience\nThis workshop is for you if you:\n\nhave used tidymodels packages like recipes, rsample, and parsnip.\nare comfortable with tidyverse syntax (e.g. piping, mutates, pivoting), and\nhave some experience with resampling and modeling (e.g., linear regression, random forests, etc.), but we don’t expect you to be an expert in these.\n\nParticipants who are new to tidymodels will benefit from taking the Introduction to tidymodels workshop before joining this one.\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nMax Kuhn is a software engineer at Posit. He is responsible for the tidymodels ecosystem and maintains about 30 packages, including caret. He was applying models in the pharmaceutical and diagnostic industries for over 18 years. Max has a Ph.D. in Biostatistics. He, and Kjell Johnson, wrote the book Applied Predictive Modeling, which won the Ziegel award from the American Statistical Association. Their second book, Feature Engineering and Selection, was published in 2019 and the book Tidy Models with R was published in 2022."
  },
  {
    "objectID": "workshops/forecasting/index.html",
    "href": "workshops/forecasting/index.html",
    "title": "Tidy time series and forecasting in R",
    "section": "",
    "text": "Description\nIt is common for organizations to collect huge amounts of data over time, and existing time series analysis tools are not always suitable to handle the scale, frequency and structure of the data collected. In this workshop, we will look at some packages and methods that have been developed to handle the analysis of large collections of time series.\nOn day 1, we will look at the tsibble data structure for flexibly managing collections of related time series. We will look at how to do data wrangling, data visualizations and exploratory data analysis. We will explore feature-based methods to explore time series data in high dimensions. A similar feature-based approach can be used to identify anomalous time series within a collection of time series, or to cluster or classify time series. Primary packages for day 1 will be tsibble, lubridate and feasts (along with the tidyverse of course).\nDay 2 will be about forecasting. We will look at some classical time series models and how they are automated in the fable package, and we will explore the creation of ensemble forecasts and hybrid forecasts. Best practices for evaluating forecast accuracy will also be covered. Finally, we will look at forecast reconciliation, allowing millions of time series to be forecast in a relatively short time while accounting for constraints on how the series are related.\n\n\nAudience\nThis course is for you if you:\n\nalready use the tidyverse packages in R such as dplyr, tidyr, tibble and ggplot2,\nneed to analyze large collections of related time series, and\nwould like to learn how to use some tidy tools for time series analysis including visualization, decomposition and forecasting.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nRob J Hyndman is Professor of Statistics in the Department of Econometrics and Business Statistics at Monash University. From 2005 to 2018 he was Editor-in-Chief of the International Journal of Forecasting and a Director of the International Institute of Forecasters. Rob is the author of over 200 research papers and 5 books in statistical science. He is an elected Fellow of both the Australian Academy of Science and the Academy of Social Sciences in Australia. In 2007, he received the Moran medal from the Australian Academy of Science for his contributions to statistical research, especially in the area of statistical forecasting. In 2021, he received the Pitman medal from the Statistical Society of Australia. For over 30 years, Rob has maintained an active consulting practice, assisting hundreds of companies and organizations around the world. He has won awards for his research, teaching, consulting and graduate supervision."
  },
  {
    "objectID": "workshops/quarto-r-docs/index.html",
    "href": "workshops/quarto-r-docs/index.html",
    "title": "Introduction to Quarto with R and RStudio: Documents and Presentations",
    "section": "",
    "text": "Description\nThis workshop will prepare you to author a rich array of documents in Quarto, the next generation of R Markdown. Quarto is an open-source scientific and technical publishing system that offers multilingual programming language support to create dynamic and static documents, books, presentations, blogs, and other online resources.\nThe focus for this workshop will be on single documents. You will learn to create static documents, to add interactivity to them with Shiny and htmlwidgets, or steer them in the direction of sophisticated scientific documents. In the afternoon you’ll take the same authoring approaches to create slide presentations in various formats such as reveal.js, beamer, and pptx.\n\n\nAudience\nThis course is for you if you:\n\nhave a basic knowledge of how to use the RStudio IDE,\nhave experience with R Markdown, or\nare excited to author a range of reproducible computational documents types, from static files to slides.\n\nSeasoned users of R Markdown will get more out of the Advanced Quarto with R and RStudio: Projects, Websites, Books, and More workshop, which is focused on projects, a distinct strength of Quarto in authoring work that spans multiple documents.\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nAndrew Bray is an Associate Teaching Professor in the Department of Statistics at UC Berkeley where he develops and teaches courses in statistics and data science. His research interests include statistical computing, data privacy, and applications of statistical models to solve real world problems. He was previously an Associate Professor of Statistics in the Department of Mathematics at Reed College and an NSF Five Colleges postdoctoral fellow in western Massachusetts."
  },
  {
    "objectID": "workshops/wtf/index.html",
    "href": "workshops/wtf/index.html",
    "title": "WTF: Project oriented workflows with Git and GitHub",
    "section": "",
    "text": "Description\nThis 1 day What They Forgot (WTF) To Teach You About R workshop is for experienced R and RStudio users who want to (re)design their R lifestyle via project-oriented workflows and version control for data science (Git/GitHub). At the conclusion of the workshop you will have strategies for organizing data science projects and workflows, employing robust file paths, constructing human and machine readable file names, and facilitating collaboration with yourself or others via version control.\n\n\nAudience\nThis course is for you if you answer yes to these questions:\n\nHave you been using R for a while and feel there might be better ways to organize your R life, but don’t know what they are?\nDo you want to put programming on pause and learn about actionable programming-adjacent workflows for streamlining analysis in R?\nAre you willing to feel a bit of (git) pain to leverage the benefits of version control for collaboration and time travel?\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nShannon Pileggi PhD (she/her) is a Lead Data Scientist at The Prostate Cancer Clinical Trials Consortium, a frequent blogger, and a member of the R-Ladies Global leadership team. She enjoys automating data wrangling and data outputs, and making both data insights and learning new material digestible.\n\n\n\n\nDavid Aja is a Solutions Engineer at Posit. Before joining Posit, he worked as a data scientist in the public sector."
  },
  {
    "objectID": "workshops/causal-inference/index.html",
    "href": "workshops/causal-inference/index.html",
    "title": "Causal Inference with R",
    "section": "",
    "text": "Description\nIn this workshop, we’ll teach the essential elements of answering causal questions in R through causal diagrams, and causal modeling techniques such as propensity scores and inverse probability weighting.\nIn both data science and academic research, prediction modeling is often not enough; to answer many questions, we need to approach them causally. In this workshop, we’ll teach the essential elements of answering causal questions in R through causal diagrams, and causal modeling techniques such as propensity scores and inverse probability weighting. We’ll also show that by distinguishing predictive models from causal models, we can better take advantage of both tools. You’ll be able to use the tools you already know--the tidyverse, regression models, and more--to answer the questions that are important to your work.\n\n\nAudience\nThis course is for you if you:\n\nknow how to fit a linear regression model in R,\nhave a basic understanding of data manipulation and visualization using tidyverse tools, and\nare interested in understanding the fundamentals behind how to move from estimating correlations to causal relationships.\n\n\n\nInstructors\n\n\n\n\n\n\n\n\n\n\nMalcolm Barrett is a data scientist and an epidemiologist. During his Ph.D., he studied vision loss, focusing on epidemiologic methods. He’s since worked in the private sector, including Teladoc Health and Apple. Malcolm is also the author of several causal inference-focused R packages, such as ggdag and tidysmd. He regularly contributes to other open source software, including favorite community projects like usethis, ggplot2, R Markdown.\n\n\n\n\nTravis Gerke is a post-academic leading data science teams for clinical trials. In his graduate training (AM Biostatistics and ScD Epidemiology, Harvard), Travis was fortunate to learn from key leaders in the causal inference domain, and he is thrilled to pass along relevant insights in this workshop. Like Malcolm, Travis enjoys developing R packages (shinyDAG, ggconsort, ggswimlane), and is an active member of the wonderful R community."
  },
  {
    "objectID": "workshops/shiny-production/index.html",
    "href": "workshops/shiny-production/index.html",
    "title": "Shiny in production: Tools and Techniques",
    "section": "",
    "text": "Description\nShiny brings tremendous possibilities to share innovative data science workflows with others inside an intuitive web interface. Many in the Shiny community have shared effective development techniques for building a robust application. Even with the best intentions during application development, a myriad of issues can arise once it leaves the confines of your machine.  In this one-day workshop, you will implement core techniques to account for common scenarios that arise once your application is used in production, such as accounting for thousands of simultaneous users, how effective profiling can address performance bottlenecks, and ensuring your application is doing as little as possible to ensure a smooth and responsive experience.  \n\n\nAudience\nThis course assumes intermediate knowledge of building Shiny applications in R and prior experience deploying an application to a platform such as the shinyapps.io service or products like Posit Connect.\nThis course is for you if you:\n\nhad a Shiny application work just fine on your machine, but encounters critical issues after deployment,\nare eager to prospectively apply techniques before deployment to plan for the unexpected, and\nwant to know the benefits and trade-offs between various ways of hosting Shiny applications.\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\nEric Nantz is a director within the statistical innovation center at Eli Lilly and Company, creating analytical pipelines and capabilities of advanced statistical methodologies for clinical design used in multiple phases of development. Outside of his day job, Eric is passionate about connecting with and showcasing the brilliant R community in multiple ways. You may recognize his voice from the R-Podcast that he launched in 2012. Eric is also the creator of the Shiny Developer Series where he interviews authors of Shiny-related packages and practitioners developing applications, as well as sharing his own R and Shiny adventures via live streams on his Twitch channel. In addition, Eric is a curator for the RWeekly project and co-host of the RWeekly Highlights podcast which accompanies every issue."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "posit::conf(2023)",
    "section": "",
    "text": "Day 1\nDay 2\n\n\n\n\n2-day workshops\n\n\n\n\n\n1\n\n\nAdministering the Posit Professional Products\n\n\n\n\n\n\n\n2\n\n\nCausal inference with R\n\n\n\n\n\n\n\n3\n\n\nIntroduction to Data Science with R\n\n\n\n\n\n\n\n4\n\n\nTidy time series and forecasting in R\n\n\n\n\n\n\n1-day workshops\n\n\n\n\n\n1\n\n\nAdvanced tidymodels\n\n\nAdvanced tidymodels\n\n\n\n\n2\n\n\nBig Data with Arrow\n\n\nTeaching a modern data science course\n\n\n\n\n3\n\n\nData Science Workflows with Posit Tools - R Focus\n\n\nData Science Workflows with Posit Tools - Python Focus\n\n\n\n\n4\n\n\nDesigning Data Visualizations to Successfully Tell a Story\n\n\nEngaging and Beautiful Data Visualizations with ggplot2\n\n\n\n\n5\n\n\nFundamentals of Package Development\n\n\nFundamentals of Package Development\n\n\n\n\n6\n\n\nGetting Started with Shiny for Python\n\n\nEnhancing Communication & Collaboration in Data Science with Quarto and Jupyter Notebooks\n\n\n\n\n7\n\n\nGetting Started with Shiny for R\n\n\nShiny Dashboards\n\n\n\n\n8\n\n\nIntroduction to Data Science with Python\n\n\nMachine Learning and Deep Learning with Python\n\n\n\n\n9\n\n\nIntroduction to Quarto with R + RStudio: Documents and presentations\n\n\nAdvanced Quarto with R + RStudio: Projects, websites, books, and more\n\n\n\n\n10\n\n\nIntroduction to tidymodels\n\n\nDeploy and maintain models with vetiver\n\n\n\n\n11\n\n\nIt's not just code: managing an open source project\n\n\nSteal like an Rtist: Creative Coding in R\n\n\n\n\n12\n\n\nPackage Development Masterclass\n\n\nPackage Development Masterclass\n\n\n\n\n13\n\n\nWeb Design for Shiny Developers\n\n\nShiny in production: Tools and Techniques\n\n\n\n\n14\n\n\nWhat they forgot to teach you about R\n\n\nFrom R User to R programmer"
  }
]